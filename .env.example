## AlphaCore prompt generation (LLM)
#
# Set `OPENAI_API_KEY` to enable LLM-backed prompt generation (otherwise the
# deterministic fallback prompt generator is used).
#
# OPENAI_API_KEY=...
#
# Optional (for local OpenAI-compatible servers):
# OPENAI_BASE_URL=http://localhost:11434/v1
#
# Model + settings:
# ALPHACORE_TASK_PROMPT_MODEL=gpt-5-mini
# ALPHACORE_LLM_TEMPERATURE=0.6
# ALPHACORE_LLM_RETRIES=3
# Max output tokens for the prompt (newer models may enforce different limits):
# ALPHACORE_LLM_MAX_OUTPUT_TOKENS=4096
#
# Force enable/disable:
# ALPHACORE_ENABLE_LLM=true
#
# If LLM generation fails after retries:
# ALPHACORE_LLM_FALLBACK=true
#
# Some deployments/models may only support one API; this setting can force
# which API the generator uses (default is auto, which tries chat first then responses):
# ALPHACORE_LLM_API=responses
